{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Semantic Frame","text":"<p>The \"JSON.stringify()\" for the Agentic Age.</p> <p>Semantic Frame is a Python library that converts raw numerical data (NumPy, Pandas, Polars) into token-efficient natural language descriptions.</p> <p>It acts as a \"Semantic Bridge\" between your data and your AI Agents.</p>"},{"location":"#the-problem-rag-for-numbers","title":"The Problem: \"RAG for Numbers\"","text":"<p>LLMs are terrible at arithmetic. When you send raw data like <code>[100, 102, 99, 101, 500]</code> to GPT-4:</p> <ul> <li>Token Waste: 10,000 data points = ~20,000 tokens.</li> <li>Hallucination Risk: LLMs guess trends instead of calculating them.</li> <li>Context Overflow: Large datasets fill the context window.</li> </ul>"},{"location":"#the-solution","title":"The Solution","text":"<p>Semantic Frame performs deterministic analysis using NumPy/SciPy, then translates the results into a concise narrative.</p> <pre><code>from semantic_frame import describe_series\nimport pandas as pd\n\ndata = pd.Series([100, 102, 99, 101, 500, 100, 98])\nprint(describe_series(data, context=\"Server Latency\"))\n</code></pre> <p>Output:</p> <p>\"The Server Latency data shows a flat/stationary pattern with stable variability. 1 anomaly detected at index 4 (value: 500.00). Baseline: 100.00.\"</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Universal Dictionary: Standardized terms for Trend, Volatility, and Seasonality.</li> <li>Deterministic Math: No LLM hallucinations. Math is done by NumPy.</li> <li>Framework Agnostic: Works with Pandas, Polars, NumPy, and Python lists.</li> <li>Agent Ready: Integrates with LangChain, CrewAI, and ElizaOS.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install semantic-frame\n</code></pre> <p>Get Started{ .md-button .md-button--primary }</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#core-functions","title":"Core Functions","text":""},{"location":"api/#semantic_frame.main","title":"<code>semantic_frame.main</code>","text":"<p>Main entry point for Semantic Frame.</p> <p>This module provides the primary API for converting numerical data into semantic descriptions.</p> Usage <p>import pandas as pd from semantic_frame import describe_series</p> <p>data = pd.Series([100, 102, 99, 101, 500, 100, 98]) print(describe_series(data, context=\"Server Latency (ms)\")) \"The Server Latency (ms) data shows a flat/stationary pattern...\"</p>"},{"location":"api/#semantic_frame.main.compression_stats","title":"<code>compression_stats(original_data, result)</code>","text":"<p>Calculate detailed compression statistics.</p> <p>Parameters:</p> Name Type Description Default <code>original_data</code> <code>ArrayLike</code> <p>The original input data.</p> required <code>result</code> <code>SemanticResult</code> <p>The SemanticResult from describe_series.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict with compression statistics.</p> Source code in <code>semantic_frame/main.py</code> <pre><code>def compression_stats(original_data: ArrayLike, result: SemanticResult) -&gt; dict[str, Any]:\n    \"\"\"Calculate detailed compression statistics.\n\n    Args:\n        original_data: The original input data.\n        result: The SemanticResult from describe_series.\n\n    Returns:\n        Dict with compression statistics.\n    \"\"\"\n    values = _to_numpy(original_data)\n\n    # Estimate original token count (rough: 2 tokens per number)\n    original_tokens = len(values) * 2\n\n    # Narrative tokens (rough: 1 token per word)\n    narrative_tokens = len(result.narrative.split())\n\n    # JSON output tokens\n    json_str = result.to_json_str()\n    json_tokens = len(json_str.split())\n\n    return {\n        \"original_data_points\": len(values),\n        \"original_tokens_estimate\": original_tokens,\n        \"narrative_tokens\": narrative_tokens,\n        \"json_tokens\": json_tokens,\n        \"narrative_compression_ratio\": 1 - (narrative_tokens / max(original_tokens, 1)),\n        \"json_compression_ratio\": 1 - (json_tokens / max(original_tokens, 1)),\n    }\n</code></pre>"},{"location":"api/#semantic_frame.main.describe_dataframe","title":"<code>describe_dataframe(df, context=None, correlation_threshold=0.5)</code>","text":"<p>Analyze all numeric columns in a DataFrame with correlation analysis.</p> <p>Runs describe_series on each numeric column and detects cross-column correlations to identify relationships like \"Sales UP, Inventory DOWN\".</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrameLike</code> <p>Input DataFrame (pandas or polars).</p> required <code>context</code> <code>str | None</code> <p>Optional context prefix. Column names will be appended.</p> <code>None</code> <code>correlation_threshold</code> <code>float</code> <p>Minimum |r| for correlation reporting (default 0.5). Only correlations with absolute value &gt;= threshold are included.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>DataFrameResult</code> <p>DataFrameResult with per-column analysis and correlation insights.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'sales': [100, 200, 300, 400, 500],\n...     'inventory': [500, 400, 300, 200, 100],\n... })\n&gt;&gt;&gt; result = describe_dataframe(df, context=\"Retail Metrics\")\n&gt;&gt;&gt; print(result.summary_narrative)\n'Analyzed 2 numeric column(s) in Retail Metrics...'\n&gt;&gt;&gt; for corr in result.correlations:\n...     print(corr.narrative)\n'inventory and sales are strongly inverse (r=-1.00)'\n</code></pre> Source code in <code>semantic_frame/main.py</code> <pre><code>def describe_dataframe(\n    df: DataFrameLike,\n    context: str | None = None,\n    correlation_threshold: float = 0.5,\n) -&gt; DataFrameResult:\n    \"\"\"Analyze all numeric columns in a DataFrame with correlation analysis.\n\n    Runs describe_series on each numeric column and detects cross-column\n    correlations to identify relationships like \"Sales UP, Inventory DOWN\".\n\n    Args:\n        df: Input DataFrame (pandas or polars).\n        context: Optional context prefix. Column names will be appended.\n        correlation_threshold: Minimum |r| for correlation reporting (default 0.5).\n            Only correlations with absolute value &gt;= threshold are included.\n\n    Returns:\n        DataFrameResult with per-column analysis and correlation insights.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; df = pd.DataFrame({\n        ...     'sales': [100, 200, 300, 400, 500],\n        ...     'inventory': [500, 400, 300, 200, 100],\n        ... })\n        &gt;&gt;&gt; result = describe_dataframe(df, context=\"Retail Metrics\")\n        &gt;&gt;&gt; print(result.summary_narrative)\n        'Analyzed 2 numeric column(s) in Retail Metrics...'\n        &gt;&gt;&gt; for corr in result.correlations:\n        ...     print(corr.narrative)\n        'inventory and sales are strongly inverse (r=-1.00)'\n    \"\"\"\n    column_results: dict[str, SemanticResult] = {}\n    values_dict: dict[str, np.ndarray] = {}\n\n    # Detect Polars vs Pandas\n    module_name = type(df).__module__\n\n    if \"polars\" in module_name:\n        # Polars DataFrame\n        for col_name in df.columns:\n            dtype = df[col_name].dtype\n            # Check if numeric (int, float types)\n            if dtype.is_numeric():\n                col_context = f\"{context} - {col_name}\" if context else col_name\n                values = df[col_name].to_numpy()\n                values_dict[col_name] = values.astype(float)\n                result = describe_series(\n                    df[col_name],\n                    context=col_context,\n                    output=\"full\",\n                )\n                column_results[col_name] = result  # type: ignore\n    else:\n        # Pandas DataFrame\n        numeric_cols = df.select_dtypes(include=[np.number]).columns  # type: ignore\n        for col_name in numeric_cols:\n            col_context = f\"{context} - {col_name}\" if context else str(col_name)\n            values = df[col_name].to_numpy()  # type: ignore\n            values_dict[str(col_name)] = values.astype(float)\n            result = describe_series(\n                df[col_name],  # type: ignore\n                context=col_context,\n                output=\"full\",\n            )\n            column_results[str(col_name)] = result  # type: ignore\n\n    # Calculate correlations\n    correlation_matrix = calc_correlation_matrix(values_dict)\n    significant = identify_significant_correlations(\n        correlation_matrix, threshold=correlation_threshold\n    )\n\n    # Build CorrelationInsight objects\n    correlation_insights: list[CorrelationInsight] = []\n    key_insights: list[str] = []\n\n    for col_a, col_b, r, state in significant:\n        narrative = generate_correlation_narrative(col_a, col_b, r, state)\n        correlation_insights.append(\n            CorrelationInsight(\n                column_a=col_a,\n                column_b=col_b,\n                correlation=r,\n                state=state,\n                narrative=narrative,\n            )\n        )\n        key_insights.append(narrative)\n\n    # Generate summary narrative\n    summary = generate_dataframe_summary(\n        column_count=len(column_results),\n        significant_correlations=len(correlation_insights),\n        key_insights=key_insights,\n        context=context,\n    )\n\n    return DataFrameResult(\n        columns=column_results,\n        correlations=tuple(correlation_insights),\n        summary_narrative=summary,\n    )\n</code></pre>"},{"location":"api/#semantic_frame.main.describe_series","title":"<code>describe_series(data, context=None, output='text')</code>","text":"<pre><code>describe_series(data: ArrayLike, context: str | None = None, output: Literal['text'] = 'text') -&gt; str\n</code></pre><pre><code>describe_series(data: ArrayLike, context: str | None = None, output: Literal['json'] = ...) -&gt; dict[str, Any]\n</code></pre><pre><code>describe_series(data: ArrayLike, context: str | None = None, output: Literal['full'] = ...) -&gt; SemanticResult\n</code></pre> <p>Convert a data series into a semantic description.</p> <p>This is the primary API for analyzing single-column data. It converts raw numerical data into token-efficient natural language descriptions suitable for LLM context.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ArrayLike</code> <p>Input data. Supports: - pandas.Series - numpy.ndarray - polars.Series - Python list of numbers</p> required <code>context</code> <code>str | None</code> <p>Optional context label for the data (e.g., \"CPU Usage\",     \"Sales Data\", \"Temperature Readings\"). Used in narrative.</p> <code>None</code> <code>output</code> <code>str</code> <p>Output format: - \"text\": Returns narrative string only (default) - \"json\": Returns dict suitable for JSON serialization - \"full\": Returns complete SemanticResult object</p> <code>'text'</code> <p>Returns:</p> Type Description <code>str | dict[str, Any] | SemanticResult</code> <p>Semantic description in the requested format.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If data is not a supported type or contains non-numeric values.</p> <code>ValueError</code> <p>If output format is not valid.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; data = pd.Series([100, 102, 99, 101, 500, 100, 98])\n</code></pre> <pre><code>&gt;&gt;&gt; # Get narrative text (default)\n&gt;&gt;&gt; describe_series(data, context=\"Server Latency (ms)\")\n'The Server Latency (ms) data shows a flat/stationary pattern...'\n</code></pre> <pre><code>&gt;&gt;&gt; # Get structured JSON\n&gt;&gt;&gt; describe_series(data, output=\"json\")\n{'narrative': '...', 'trend': 'flat/stationary', ...}\n</code></pre> <pre><code>&gt;&gt;&gt; # Get full result object\n&gt;&gt;&gt; result = describe_series(data, output=\"full\")\n&gt;&gt;&gt; print(result.compression_ratio)\n0.95\n</code></pre> Source code in <code>semantic_frame/main.py</code> <pre><code>def describe_series(\n    data: ArrayLike,\n    context: str | None = None,\n    output: str = \"text\",\n) -&gt; str | dict[str, Any] | SemanticResult:\n    \"\"\"Convert a data series into a semantic description.\n\n    This is the primary API for analyzing single-column data. It converts\n    raw numerical data into token-efficient natural language descriptions\n    suitable for LLM context.\n\n    Args:\n        data: Input data. Supports:\n            - pandas.Series\n            - numpy.ndarray\n            - polars.Series\n            - Python list of numbers\n        context: Optional context label for the data (e.g., \"CPU Usage\",\n                \"Sales Data\", \"Temperature Readings\"). Used in narrative.\n        output: Output format:\n            - \"text\": Returns narrative string only (default)\n            - \"json\": Returns dict suitable for JSON serialization\n            - \"full\": Returns complete SemanticResult object\n\n    Returns:\n        Semantic description in the requested format.\n\n    Raises:\n        TypeError: If data is not a supported type or contains non-numeric values.\n        ValueError: If output format is not valid.\n\n    Examples:\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; data = pd.Series([100, 102, 99, 101, 500, 100, 98])\n\n        &gt;&gt;&gt; # Get narrative text (default)\n        &gt;&gt;&gt; describe_series(data, context=\"Server Latency (ms)\")\n        'The Server Latency (ms) data shows a flat/stationary pattern...'\n\n        &gt;&gt;&gt; # Get structured JSON\n        &gt;&gt;&gt; describe_series(data, output=\"json\")\n        {'narrative': '...', 'trend': 'flat/stationary', ...}\n\n        &gt;&gt;&gt; # Get full result object\n        &gt;&gt;&gt; result = describe_series(data, output=\"full\")\n        &gt;&gt;&gt; print(result.compression_ratio)\n        0.95\n    \"\"\"\n    # Validate output format\n    if output not in VALID_OUTPUT_FORMATS:\n        raise ValueError(\n            f\"Invalid output format: {output!r}. \" f\"Expected one of: {VALID_OUTPUT_FORMATS}\"\n        )\n\n    # Convert to numpy\n    values = _to_numpy(data)\n\n    # Run analysis\n    result = analyze_series(values, context=context)\n\n    # Return in requested format\n    if output == \"text\":\n        return result.narrative\n    elif output == \"json\":\n        return result.model_dump(mode=\"json\", by_alias=True)\n    else:  # output == \"full\"\n        return result\n</code></pre>"},{"location":"api/#analyzers-the-math","title":"Analyzers (The Math)","text":""},{"location":"api/#semantic_frame.core.analyzers","title":"<code>semantic_frame.core.analyzers</code>","text":"<p>Mathematical analysis functions for data profiling.</p> <p>This module contains the core statistical functions that power the semantic analysis. All calculations are deterministic (NumPy-based) - no LLM involvement.</p> <p>Key functions: - calc_linear_slope: Trend direction analysis - classify_trend: Classify trend based on normalized slope - calc_volatility: Variability measurement using coefficient of variation - detect_anomalies: Outlier detection with adaptive IQR/Z-score methods - classify_anomaly_state: Classify anomaly severity - assess_data_quality: Assess data completeness - calc_seasonality: Cyclic pattern detection via autocorrelation - calc_distribution_shape: Distribution classification via skewness/kurtosis</p>"},{"location":"api/#semantic_frame.core.analyzers.assess_data_quality","title":"<code>assess_data_quality(values)</code>","text":"<p>Assess data quality based on missing value percentage.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>NumPy array (may contain NaN values).</p> required <p>Returns:</p> Type Description <code>tuple[float, DataQuality]</code> <p>Tuple of (missing_percentage, DataQuality).</p> Source code in <code>semantic_frame/core/analyzers.py</code> <pre><code>def assess_data_quality(values: np.ndarray) -&gt; tuple[float, DataQuality]:\n    \"\"\"Assess data quality based on missing value percentage.\n\n    Args:\n        values: NumPy array (may contain NaN values).\n\n    Returns:\n        Tuple of (missing_percentage, DataQuality).\n    \"\"\"\n    if len(values) == 0:\n        return 100.0, DataQuality.FRAGMENTED\n\n    missing_pct = float(np.sum(np.isnan(values)) / len(values) * 100)\n\n    if missing_pct &lt; 1:\n        return missing_pct, DataQuality.PRISTINE\n    if missing_pct &lt; 5:\n        return missing_pct, DataQuality.GOOD\n    if missing_pct &lt; 20:\n        return missing_pct, DataQuality.SPARSE\n    return missing_pct, DataQuality.FRAGMENTED\n</code></pre>"},{"location":"api/#semantic_frame.core.analyzers.calc_distribution_shape","title":"<code>calc_distribution_shape(values)</code>","text":"<p>Analyze distribution shape using skewness and kurtosis.</p> Thresholds <ul> <li>UNIFORM: kurtosis &lt; -1.2 and |skewness| &lt; 0.3</li> <li>BIMODAL: kurtosis &lt; -1 (flat-topped distribution)</li> <li>NORMAL: |skewness| &lt; 0.5</li> <li>LEFT_SKEWED: skewness &lt; -0.5</li> <li>RIGHT_SKEWED: skewness &gt; 0.5</li> </ul> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>NumPy array of numerical values (no NaN).</p> required <p>Returns:</p> Type Description <code>DistributionShape</code> <p>DistributionShape classification.</p> Source code in <code>semantic_frame/core/analyzers.py</code> <pre><code>def calc_distribution_shape(values: np.ndarray) -&gt; DistributionShape:\n    \"\"\"Analyze distribution shape using skewness and kurtosis.\n\n    Thresholds:\n        - UNIFORM: kurtosis &lt; -1.2 and |skewness| &lt; 0.3\n        - BIMODAL: kurtosis &lt; -1 (flat-topped distribution)\n        - NORMAL: |skewness| &lt; 0.5\n        - LEFT_SKEWED: skewness &lt; -0.5\n        - RIGHT_SKEWED: skewness &gt; 0.5\n\n    Args:\n        values: NumPy array of numerical values (no NaN).\n\n    Returns:\n        DistributionShape classification.\n    \"\"\"\n    if len(values) &lt; 4:\n        return DistributionShape.NORMAL\n\n    # Check for constant data (all same values) - skip scipy to avoid warnings\n    if np.ptp(values) == 0:\n        return DistributionShape.NORMAL\n\n    try:\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\", RuntimeWarning)\n            s = float(skew(values))\n            k = float(kurtosis(values))\n    except (ValueError, FloatingPointError, RuntimeWarning) as e:\n        # scipy failed on this data - log and return safe default\n        logger.debug(\n            \"Distribution shape calculation failed for array of length %d: %s\",\n            len(values),\n            str(e),\n        )\n        return DistributionShape.NORMAL\n\n    # Handle NaN results from near-constant data\n    if np.isnan(s) or np.isnan(k):\n        logger.debug(\n            \"Distribution calculation returned NaN (skew=%s, kurtosis=%s), \" \"defaulting to NORMAL\",\n            s,\n            k,\n        )\n        return DistributionShape.NORMAL\n\n    # Check for uniform first (more restrictive: very negative kurtosis AND low skewness)\n    # Uniform distributions have kurtosis \u2248 -1.2 and near-zero skewness\n    if k &lt; -1.2 and abs(s) &lt; 0.3:\n        return DistributionShape.UNIFORM\n\n    # Check for bimodality (negative excess kurtosis suggests flatter/bimodal)\n    # Note: True bimodality requires different analysis; this detects flat-topped distributions\n    if k &lt; -1:\n        return DistributionShape.BIMODAL\n\n    # Classify by skewness\n    if abs(s) &lt; 0.5:\n        return DistributionShape.NORMAL\n    if s &lt; -0.5:\n        return DistributionShape.LEFT_SKEWED\n    return DistributionShape.RIGHT_SKEWED\n</code></pre>"},{"location":"api/#semantic_frame.core.analyzers.calc_linear_slope","title":"<code>calc_linear_slope(values)</code>","text":"<p>Calculate normalized linear regression slope.</p> <p>The slope is normalized by data range and length to make it scale-independent. This allows consistent threshold comparisons across datasets with different magnitudes.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>NumPy array of numerical values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Normalized slope value. Positive = upward trend, negative = downward.</p> Source code in <code>semantic_frame/core/analyzers.py</code> <pre><code>def calc_linear_slope(values: np.ndarray) -&gt; float:\n    \"\"\"Calculate normalized linear regression slope.\n\n    The slope is normalized by data range and length to make it\n    scale-independent. This allows consistent threshold comparisons\n    across datasets with different magnitudes.\n\n    Args:\n        values: NumPy array of numerical values.\n\n    Returns:\n        Normalized slope value. Positive = upward trend, negative = downward.\n    \"\"\"\n    if len(values) &lt; 2:\n        return 0.0\n\n    x = np.arange(len(values))\n    # Use polyfit for simple linear regression\n    slope = np.polyfit(x, values, 1)[0]\n\n    # Normalize by data range to make scale-independent\n    data_range = float(np.ptp(values))\n    if data_range == 0:\n        return 0.0\n\n    return float(slope * len(values) / data_range)\n</code></pre>"},{"location":"api/#semantic_frame.core.analyzers.calc_seasonality","title":"<code>calc_seasonality(values, max_lag=30)</code>","text":"<p>Detect seasonality using autocorrelation analysis.</p> <p>Calculates autocorrelation at various lags to detect cyclic patterns.</p> <p>Thresholds (peak autocorrelation):     - NONE: &lt; 0.3     - WEAK: 0.3 - 0.5     - MODERATE: 0.5 - 0.7     - STRONG: &gt;= 0.7</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>NumPy array of time-ordered values.</p> required <code>max_lag</code> <code>int</code> <p>Maximum lag to check (default 30).</p> <code>30</code> <p>Returns:</p> Type Description <code>tuple[float, SeasonalityState]</code> <p>Tuple of (peak_autocorrelation, SeasonalityState).</p> Source code in <code>semantic_frame/core/analyzers.py</code> <pre><code>def calc_seasonality(values: np.ndarray, max_lag: int = 30) -&gt; tuple[float, SeasonalityState]:\n    \"\"\"Detect seasonality using autocorrelation analysis.\n\n    Calculates autocorrelation at various lags to detect cyclic patterns.\n\n    Thresholds (peak autocorrelation):\n        - NONE: &lt; 0.3\n        - WEAK: 0.3 - 0.5\n        - MODERATE: 0.5 - 0.7\n        - STRONG: &gt;= 0.7\n\n    Args:\n        values: NumPy array of time-ordered values.\n        max_lag: Maximum lag to check (default 30).\n\n    Returns:\n        Tuple of (peak_autocorrelation, SeasonalityState).\n    \"\"\"\n    n = len(values)\n\n    if n &lt; 4:\n        return 0.0, SeasonalityState.NONE\n\n    # Check for constant data - no seasonality possible\n    if np.ptp(values) == 0:\n        return 0.0, SeasonalityState.NONE\n\n    # Adjust max_lag for short series\n    effective_max_lag = min(max_lag, n // 2)\n    if effective_max_lag &lt; 2:\n        return 0.0, SeasonalityState.NONE\n\n    # Calculate autocorrelation at different lags\n    autocorrs: list[float] = []\n    logged_error = False\n    for lag in range(1, effective_max_lag):\n        try:\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                corr, _ = pearsonr(values[:-lag], values[lag:])\n            if not np.isnan(corr):\n                autocorrs.append(abs(float(corr)))\n        except (ValueError, FloatingPointError) as e:\n            # Log first failure, then continue silently\n            if not logged_error:\n                logger.debug(\n                    \"Seasonality calculation had errors at lag %d: %s \"\n                    \"(continuing with remaining lags)\",\n                    lag,\n                    str(e),\n                )\n                logged_error = True\n            continue\n\n    if not autocorrs:\n        return 0.0, SeasonalityState.NONE\n\n    peak_autocorr = max(autocorrs)\n\n    if peak_autocorr &lt; 0.3:\n        return peak_autocorr, SeasonalityState.NONE\n    if peak_autocorr &lt; 0.5:\n        return peak_autocorr, SeasonalityState.WEAK\n    if peak_autocorr &lt; 0.7:\n        return peak_autocorr, SeasonalityState.MODERATE\n    return peak_autocorr, SeasonalityState.STRONG\n</code></pre>"},{"location":"api/#semantic_frame.core.analyzers.calc_volatility","title":"<code>calc_volatility(values)</code>","text":"<p>Calculate volatility using coefficient of variation.</p> <p>CV = std / |mean|, providing a scale-independent measure of variability.</p> Thresholds <ul> <li>&lt; 0.05: COMPRESSED</li> <li>&lt; 0.15: STABLE</li> <li>&lt; 0.30: MODERATE</li> <li>&lt; 0.50: EXPANDING</li> <li> <p>= 0.50: EXTREME</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>NumPy array of numerical values.</p> required <p>Returns:</p> Type Description <code>tuple[float, VolatilityState]</code> <p>Tuple of (coefficient_of_variation, VolatilityState).</p> Source code in <code>semantic_frame/core/analyzers.py</code> <pre><code>def calc_volatility(values: np.ndarray) -&gt; tuple[float, VolatilityState]:\n    \"\"\"Calculate volatility using coefficient of variation.\n\n    CV = std / |mean|, providing a scale-independent measure of variability.\n\n    Thresholds:\n        - &lt; 0.05: COMPRESSED\n        - &lt; 0.15: STABLE\n        - &lt; 0.30: MODERATE\n        - &lt; 0.50: EXPANDING\n        - &gt;= 0.50: EXTREME\n\n    Args:\n        values: NumPy array of numerical values.\n\n    Returns:\n        Tuple of (coefficient_of_variation, VolatilityState).\n    \"\"\"\n    if len(values) == 0:\n        return 0.0, VolatilityState.STABLE\n\n    mean = float(np.mean(values))\n    if mean == 0:\n        # Handle zero mean - use std relative to data range\n        data_range = float(np.ptp(values))\n        if data_range == 0:\n            return 0.0, VolatilityState.COMPRESSED\n        cv = float(np.std(values) / data_range)\n    else:\n        cv = float(np.std(values) / abs(mean))\n\n    if cv &lt; 0.05:\n        return cv, VolatilityState.COMPRESSED\n    if cv &lt; 0.15:\n        return cv, VolatilityState.STABLE\n    if cv &lt; 0.30:\n        return cv, VolatilityState.MODERATE\n    if cv &lt; 0.50:\n        return cv, VolatilityState.EXPANDING\n    return cv, VolatilityState.EXTREME\n</code></pre>"},{"location":"api/#semantic_frame.core.analyzers.classify_anomaly_state","title":"<code>classify_anomaly_state(anomalies)</code>","text":"<p>Classify anomaly severity based on count and z-scores.</p> <p>Parameters:</p> Name Type Description Default <code>anomalies</code> <code>list[AnomalyInfo]</code> <p>List of detected anomalies.</p> required <p>Returns:</p> Type Description <code>AnomalyState</code> <p>AnomalyState classification.</p> Source code in <code>semantic_frame/core/analyzers.py</code> <pre><code>def classify_anomaly_state(anomalies: list[AnomalyInfo]) -&gt; AnomalyState:\n    \"\"\"Classify anomaly severity based on count and z-scores.\n\n    Args:\n        anomalies: List of detected anomalies.\n\n    Returns:\n        AnomalyState classification.\n    \"\"\"\n    if not anomalies:\n        return AnomalyState.NONE\n\n    # Check for extreme z-scores\n    max_z = max(a.z_score for a in anomalies)\n    if max_z &gt; 5.0:\n        return AnomalyState.EXTREME\n\n    count = len(anomalies)\n    if count &gt; 5:\n        return AnomalyState.EXTREME\n    if count &gt;= 3:\n        return AnomalyState.SIGNIFICANT\n    return AnomalyState.MINOR\n</code></pre>"},{"location":"api/#semantic_frame.core.analyzers.classify_trend","title":"<code>classify_trend(normalized_slope)</code>","text":"<p>Classify trend based on normalized slope value.</p> Thresholds <ul> <li> <p>0.5: RISING_SHARP</p> </li> <li> <p>0.1: RISING_STEADY</p> </li> <li>&lt; -0.5: FALLING_SHARP</li> <li>&lt; -0.1: FALLING_STEADY</li> <li>else: FLAT</li> </ul> <p>Parameters:</p> Name Type Description Default <code>normalized_slope</code> <code>float</code> <p>Output from calc_linear_slope.</p> required <p>Returns:</p> Type Description <code>TrendState</code> <p>TrendState enum value.</p> Source code in <code>semantic_frame/core/analyzers.py</code> <pre><code>def classify_trend(normalized_slope: float) -&gt; TrendState:\n    \"\"\"Classify trend based on normalized slope value.\n\n    Thresholds:\n        - &gt; 0.5: RISING_SHARP\n        - &gt; 0.1: RISING_STEADY\n        - &lt; -0.5: FALLING_SHARP\n        - &lt; -0.1: FALLING_STEADY\n        - else: FLAT\n\n    Args:\n        normalized_slope: Output from calc_linear_slope.\n\n    Returns:\n        TrendState enum value.\n    \"\"\"\n    if normalized_slope &gt; 0.5:\n        return TrendState.RISING_SHARP\n    if normalized_slope &gt; 0.1:\n        return TrendState.RISING_STEADY\n    if normalized_slope &lt; -0.5:\n        return TrendState.FALLING_SHARP\n    if normalized_slope &lt; -0.1:\n        return TrendState.FALLING_STEADY\n    return TrendState.FLAT\n</code></pre>"},{"location":"api/#semantic_frame.core.analyzers.detect_anomalies","title":"<code>detect_anomalies(values, z_threshold=3.0)</code>","text":"<p>Detect anomalies using adaptive method based on sample size.</p> <p>The threshold of 10 samples balances: - IQR method: More robust to outliers with small samples, but loses   precision with few data points for quartile estimation. - Z-score method: Requires sufficient samples for stable mean/std   estimates; unreliable below ~10 samples.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>ndarray</code> <p>NumPy array of numerical values.</p> required <code>z_threshold</code> <code>float</code> <p>Z-score threshold for large sample detection (default 3.0). Must be positive.</p> <code>3.0</code> <p>Returns:</p> Type Description <code>list[AnomalyInfo]</code> <p>List of AnomalyInfo objects for detected outliers.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If z_threshold is not positive.</p> Source code in <code>semantic_frame/core/analyzers.py</code> <pre><code>def detect_anomalies(values: np.ndarray, z_threshold: float = 3.0) -&gt; list[AnomalyInfo]:\n    \"\"\"Detect anomalies using adaptive method based on sample size.\n\n    The threshold of 10 samples balances:\n    - IQR method: More robust to outliers with small samples, but loses\n      precision with few data points for quartile estimation.\n    - Z-score method: Requires sufficient samples for stable mean/std\n      estimates; unreliable below ~10 samples.\n\n    Args:\n        values: NumPy array of numerical values.\n        z_threshold: Z-score threshold for large sample detection (default 3.0).\n            Must be positive.\n\n    Returns:\n        List of AnomalyInfo objects for detected outliers.\n\n    Raises:\n        ValueError: If z_threshold is not positive.\n    \"\"\"\n    if z_threshold &lt;= 0:\n        raise ValueError(f\"z_threshold must be positive, got {z_threshold}\")\n\n    if len(values) &lt; 3:\n        return []\n\n    if len(values) &lt; 10:\n        # IQR method for small samples - more robust with limited data\n        return _detect_anomalies_iqr(values)\n\n    return _detect_anomalies_zscore(values, z_threshold)\n</code></pre>"},{"location":"api/#enums-the-dictionary","title":"Enums (The Dictionary)","text":""},{"location":"api/#semantic_frame.core.enums","title":"<code>semantic_frame.core.enums</code>","text":"<p>Semantic vocabulary enums for data classification.</p> <p>These enums define the \"dictionary\" that translates mathematical analysis into natural language descriptions for LLM consumption.</p>"},{"location":"api/#semantic_frame.core.enums.AnomalyState","title":"<code>AnomalyState</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Classification of outlier presence and severity.</p> Thresholds <ul> <li>NONE: No outliers detected</li> <li>MINOR: 1-2 outliers</li> <li>SIGNIFICANT: 3-5 outliers</li> <li>EXTREME: &gt;5 outliers or any with z-score &gt;5</li> </ul> Source code in <code>semantic_frame/core/enums.py</code> <pre><code>class AnomalyState(str, Enum):\n    \"\"\"Classification of outlier presence and severity.\n\n    Thresholds:\n        - NONE: No outliers detected\n        - MINOR: 1-2 outliers\n        - SIGNIFICANT: 3-5 outliers\n        - EXTREME: &gt;5 outliers or any with z-score &gt;5\n    \"\"\"\n\n    NONE = \"no anomalies\"\n    MINOR = \"minor outliers\"\n    SIGNIFICANT = \"significant outliers\"\n    EXTREME = \"extreme outliers\"\n</code></pre>"},{"location":"api/#semantic_frame.core.enums.CorrelationState","title":"<code>CorrelationState</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Classification of correlation strength between two variables.</p> <p>Thresholds (Pearson r):     - STRONG_POSITIVE: r &gt; 0.7     - MODERATE_POSITIVE: 0.4 &lt; r &lt;= 0.7     - WEAK: |r| &lt;= 0.4     - MODERATE_NEGATIVE: -0.7 &lt;= r &lt; -0.4     - STRONG_NEGATIVE: r &lt; -0.7</p> Source code in <code>semantic_frame/core/enums.py</code> <pre><code>class CorrelationState(str, Enum):\n    \"\"\"Classification of correlation strength between two variables.\n\n    Thresholds (Pearson r):\n        - STRONG_POSITIVE: r &gt; 0.7\n        - MODERATE_POSITIVE: 0.4 &lt; r &lt;= 0.7\n        - WEAK: |r| &lt;= 0.4\n        - MODERATE_NEGATIVE: -0.7 &lt;= r &lt; -0.4\n        - STRONG_NEGATIVE: r &lt; -0.7\n    \"\"\"\n\n    STRONG_POSITIVE = \"strongly correlated\"\n    MODERATE_POSITIVE = \"moderately correlated\"\n    WEAK = \"weakly related\"\n    MODERATE_NEGATIVE = \"inversely related\"\n    STRONG_NEGATIVE = \"strongly inverse\"\n</code></pre>"},{"location":"api/#semantic_frame.core.enums.DataQuality","title":"<code>DataQuality</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Classification of data completeness based on missing values.</p> Thresholds <ul> <li>PRISTINE: &lt;1% missing</li> <li>GOOD: 1-5% missing</li> <li>SPARSE: 5-20% missing</li> <li>FRAGMENTED: &gt;20% missing</li> </ul> Source code in <code>semantic_frame/core/enums.py</code> <pre><code>class DataQuality(str, Enum):\n    \"\"\"Classification of data completeness based on missing values.\n\n    Thresholds:\n        - PRISTINE: &lt;1% missing\n        - GOOD: 1-5% missing\n        - SPARSE: 5-20% missing\n        - FRAGMENTED: &gt;20% missing\n    \"\"\"\n\n    PRISTINE = \"high quality\"\n    GOOD = \"good quality\"\n    SPARSE = \"sparse\"\n    FRAGMENTED = \"fragmented\"\n</code></pre>"},{"location":"api/#semantic_frame.core.enums.DistributionShape","title":"<code>DistributionShape</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Classification of data distribution shape.</p> <p>Based on skewness and kurtosis analysis.</p> Thresholds <ul> <li>UNIFORM: kurtosis &lt; -1.2 and |skewness| &lt; 0.3</li> <li>BIMODAL: kurtosis &lt; -1 (flat-topped distribution)</li> <li>NORMAL: |skewness| &lt; 0.5</li> <li>LEFT_SKEWED: skewness &lt; -0.5</li> <li>RIGHT_SKEWED: skewness &gt; 0.5</li> </ul> <p>Note: BIMODAL detection is heuristic based on kurtosis; true bimodality requires more sophisticated analysis (e.g., Hartigan's dip test).</p> Source code in <code>semantic_frame/core/enums.py</code> <pre><code>class DistributionShape(str, Enum):\n    \"\"\"Classification of data distribution shape.\n\n    Based on skewness and kurtosis analysis.\n\n    Thresholds:\n        - UNIFORM: kurtosis &lt; -1.2 and |skewness| &lt; 0.3\n        - BIMODAL: kurtosis &lt; -1 (flat-topped distribution)\n        - NORMAL: |skewness| &lt; 0.5\n        - LEFT_SKEWED: skewness &lt; -0.5\n        - RIGHT_SKEWED: skewness &gt; 0.5\n\n    Note: BIMODAL detection is heuristic based on kurtosis; true bimodality\n    requires more sophisticated analysis (e.g., Hartigan's dip test).\n    \"\"\"\n\n    NORMAL = \"normally distributed\"\n    LEFT_SKEWED = \"left-skewed\"\n    RIGHT_SKEWED = \"right-skewed\"\n    BIMODAL = \"bimodal\"\n    UNIFORM = \"uniformly distributed\"\n</code></pre>"},{"location":"api/#semantic_frame.core.enums.SeasonalityState","title":"<code>SeasonalityState</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Classification of cyclic patterns via autocorrelation.</p> <p>Thresholds (peak autocorrelation):     - NONE: &lt;0.3     - WEAK: 0.3-0.5     - MODERATE: 0.5-0.7     - STRONG: &gt;0.7</p> Source code in <code>semantic_frame/core/enums.py</code> <pre><code>class SeasonalityState(str, Enum):\n    \"\"\"Classification of cyclic patterns via autocorrelation.\n\n    Thresholds (peak autocorrelation):\n        - NONE: &lt;0.3\n        - WEAK: 0.3-0.5\n        - MODERATE: 0.5-0.7\n        - STRONG: &gt;0.7\n    \"\"\"\n\n    NONE = \"no seasonality\"\n    WEAK = \"weak cyclic pattern\"\n    MODERATE = \"moderate seasonality\"\n    STRONG = \"strong seasonality\"\n</code></pre>"},{"location":"api/#semantic_frame.core.enums.TrendState","title":"<code>TrendState</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Classification of data trend direction and intensity.</p> <p>Thresholds (normalized slope):     - RISING_SHARP: slope &gt; 0.5     - RISING_STEADY: slope &gt; 0.1     - FLAT: -0.1 &lt;= slope &lt;= 0.1     - FALLING_STEADY: slope &lt; -0.1     - FALLING_SHARP: slope &lt; -0.5</p> Source code in <code>semantic_frame/core/enums.py</code> <pre><code>class TrendState(str, Enum):\n    \"\"\"Classification of data trend direction and intensity.\n\n    Thresholds (normalized slope):\n        - RISING_SHARP: slope &gt; 0.5\n        - RISING_STEADY: slope &gt; 0.1\n        - FLAT: -0.1 &lt;= slope &lt;= 0.1\n        - FALLING_STEADY: slope &lt; -0.1\n        - FALLING_SHARP: slope &lt; -0.5\n    \"\"\"\n\n    RISING_SHARP = \"rapidly rising\"\n    RISING_STEADY = \"steadily rising\"\n    FLAT = \"flat/stationary\"\n    FALLING_STEADY = \"steadily falling\"\n    FALLING_SHARP = \"rapidly falling\"\n</code></pre>"},{"location":"api/#semantic_frame.core.enums.VolatilityState","title":"<code>VolatilityState</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Classification of data variability using coefficient of variation.</p> Thresholds <ul> <li>COMPRESSED: CV &lt; 0.05 (very low variance)</li> <li>STABLE: CV 0.05-0.15 (normal variance)</li> <li>MODERATE: CV 0.15-0.30 (elevated variance)</li> <li>EXPANDING: CV 0.30-0.50 (high variance)</li> <li>EXTREME: CV &gt; 0.50 (very high variance)</li> </ul> Source code in <code>semantic_frame/core/enums.py</code> <pre><code>class VolatilityState(str, Enum):\n    \"\"\"Classification of data variability using coefficient of variation.\n\n    Thresholds:\n        - COMPRESSED: CV &lt; 0.05 (very low variance)\n        - STABLE: CV 0.05-0.15 (normal variance)\n        - MODERATE: CV 0.15-0.30 (elevated variance)\n        - EXPANDING: CV 0.30-0.50 (high variance)\n        - EXTREME: CV &gt; 0.50 (very high variance)\n    \"\"\"\n\n    COMPRESSED = \"compressed\"\n    STABLE = \"stable\"\n    MODERATE = \"moderate\"\n    EXPANDING = \"expanding\"\n    EXTREME = \"extreme\"\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install the package using pip:</p> <pre><code>pip install semantic-frame\n</code></pre> <p>Or with <code>uv</code>:</p> <pre><code>uv add semantic-frame\n</code></pre>"},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/#1-analyze-a-single-series","title":"1. Analyze a Single Series","text":"<p>The core function is <code>describe_series</code>. It accepts a list, NumPy array, or Pandas/Polars Series.</p> <pre><code>from semantic_frame import describe_series\n\ndata = [10, 12, 11, 13, 12, 11, 100, 12]\nnarrative = describe_series(data, context=\"User Logins\")\n\nprint(narrative)\n# Output: \"The User Logins data shows a flat/stationary pattern... 1 anomaly detected...\"\n</code></pre>"},{"location":"getting-started/#2-analyze-a-dataframe","title":"2. Analyze a DataFrame","text":"<p>Use <code>describe_dataframe</code> to analyze an entire dataset at once.</p> <pre><code>import pandas as pd\nfrom semantic_frame import describe_dataframe\n\ndf = pd.DataFrame({\n    \"price\": [100, 101, 102, 103, 104],\n    \"volume\": [500, 520, 480, 510, 490]\n})\n\nresults = describe_dataframe(df)\n\nprint(results[\"price\"].narrative)\nprint(results[\"volume\"].narrative)\n</code></pre>"},{"location":"getting-started/#3-structured-output","title":"3. Structured Output","text":"<p>If you need the raw stats instead of text, request the full object:</p> <pre><code>result = describe_series(data, output=\"full\")\n\nprint(result.trend)       # TrendState.FLAT\nprint(result.volatility)  # VolatilityState.STABLE\nprint(result.anomalies)   # List of anomalies\n</code></pre>"},{"location":"integrations/","title":"Integrations","text":"<p>Semantic Frame is designed to be the \"Math Plugin\" for your AI Agents.</p>"},{"location":"integrations/#elizaos-via-mcp","title":"ElizaOS (via MCP)","text":"<p>Semantic Frame provides a Model Context Protocol (MCP) server, allowing ElizaOS (and Claude Desktop) to use it natively.</p>"},{"location":"integrations/#installation","title":"Installation","text":"<pre><code>pip install semantic-frame[mcp]\n</code></pre>"},{"location":"integrations/#usage","title":"Usage","text":"<p>Run the MCP server:</p> <pre><code>mcp run semantic_frame.integrations.mcp:mcp\n</code></pre> <p>Configure your ElizaOS character or Claude Desktop to connect to this server. The agent will now have access to the <code>describe_data</code> tool.</p>"},{"location":"integrations/#langchain","title":"LangChain","text":"<p>We provide a native LangChain tool wrapper.</p>"},{"location":"integrations/#installation_1","title":"Installation","text":"<pre><code>pip install semantic-frame[langchain]\n</code></pre>"},{"location":"integrations/#usage_1","title":"Usage","text":"<pre><code>from semantic_frame.integrations.langchain import get_semantic_tool\nfrom langchain.agents import create_openai_tools_agent\n\n# Create the tool\ntool = get_semantic_tool(context=\"Sales Data\")\n\n# Add to your agent\ntools = [tool]\n# ... initialize agent ...\n</code></pre>"},{"location":"integrations/#crewai","title":"CrewAI","text":"<p>We provide a native CrewAI tool decorator.</p>"},{"location":"integrations/#installation_2","title":"Installation","text":"<pre><code>pip install semantic-frame[crewai]\n</code></pre>"},{"location":"integrations/#usage_2","title":"Usage","text":"<pre><code>from semantic_frame.integrations.crewai import get_crewai_tool\nfrom crewai import Agent\n\n# Create the tool\nsemantic_tool = get_crewai_tool()\n\n# Add to your agent\nanalyst = Agent(\n    role=\"Data Analyst\",\n    goal=\"Analyze market trends\",\n    tools=[semantic_tool],\n    # ...\n)\n</code></pre>"},{"location":"universal-dictionary/","title":"The Universal Dictionary","text":"<p>To ensure AI Agents can reliably \"understand\" data, we map mathematical properties to a standardized vocabulary. This is the Universal Dictionary.</p> <p>Instead of raw numbers (Slope = 0.05), the Agent receives a semantic concept (<code>RISING_STEADY</code>).</p>"},{"location":"universal-dictionary/#trend","title":"Trend","text":"<p>Calculated using Linear Regression (Slope).</p> Enum Description Math Logic <code>RISING_SHARP</code> Rapid growth Slope &gt; 0.5 (Normalized) <code>RISING_STEADY</code> Consistent growth 0.1 &lt; Slope &lt;= 0.5 <code>FLAT</code> No significant trend -0.1 &lt;= Slope &lt;= 0.1 <code>FALLING_STEADY</code> Consistent decline -0.5 &lt;= Slope &lt; -0.1 <code>FALLING_SHARP</code> Rapid decline Slope &lt; -0.5"},{"location":"universal-dictionary/#volatility","title":"Volatility","text":"<p>Calculated using Coefficient of Variation (CV = StdDev / Mean).</p> Enum Description Math Logic <code>COMPRESSED</code> Extremely tight range CV &lt; 0.05 <code>STABLE</code> Normal variation 0.05 &lt;= CV &lt; 0.2 <code>MODERATE</code> Noticeable fluctuation 0.2 &lt;= CV &lt; 0.5 <code>EXPANDING</code> High volatility 0.5 &lt;= CV &lt; 1.0 <code>EXTREME</code> Chaotic / Unpredictable CV &gt;= 1.0"},{"location":"universal-dictionary/#anomalies","title":"Anomalies","text":"<p>Calculated using an adaptive approach: *   Z-Score (for normal distributions): Threshold &gt; 3.0 *   IQR (for skewed distributions): Threshold &gt; 1.5 * IQR</p> Enum Description <code>NONE</code> No outliers detected <code>MINOR</code> 1-2 outliers <code>SIGNIFICANT</code> 3-5 outliers (requires attention) <code>EXTREME</code> &gt;5 outliers (data quality issue or crisis)"},{"location":"universal-dictionary/#seasonality","title":"Seasonality","text":"<p>Calculated using Autocorrelation (ACF).</p> Enum Description <code>NONE</code> No cyclic pattern <code>WEAK</code> Faint pattern detected <code>MODERATE</code> Clear cyclic behavior <code>STRONG</code> Highly predictable cycles"}]}
